{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fac117-6c69-4c60-826c-18bd02c2e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.optical_flow import raft_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f90a52-8da8-4f99-bcf0-ed989f5441e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/bf/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/bf/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Raft_Small_Weights.C_T_V2`. You can also use `weights=Raft_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "raft_model = raft_small(pretrained=True, progress=False).to(device)\n",
    "raft_model = raft_model.eval()\n",
    "\n",
    "gamma_values = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8]  # Gamma values\n",
    "\n",
    "def transform(img):\n",
    "    return img\n",
    "\n",
    "def gamma_correction(img, gamma):\n",
    "    return TF.adjust_gamma(img, gamma)\n",
    "\n",
    "flip_types = [\"horizontal\", \"vertical\", \"both\", \"none\"]\n",
    "\n",
    "def random_flip(img, flip_type):\n",
    "    if flip_type == \"horizontal\":\n",
    "        return TF.hflip(img)\n",
    "    elif flip_type == \"vertical\":\n",
    "        return TF.vflip(img)\n",
    "    elif flip_type == \"both\":\n",
    "        return TF.hflip(TF.vflip(img))\n",
    "    return img  # No flip\n",
    "\n",
    "# {'Instrument', 'Care', 'Bubble', 'unkown', 'unknown', 'Fat', 'SoftTIssue', 'Dura', 'BF', 'SoftTissue', 'vessel', 'Vessel', 'Bone', 'LF', 'SofrTissue'}\n",
    "num_classes = 11 # Background, BF, Vessel, Instrument, Care, Bubble, Fat, Bone, LF, Dura, SoftTissue\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ë¼ë²¨ ë§¤í•‘\n",
    "class_map = {\"BF\": 1, \"Vessel\": 2, \"vessel\": 2, \"Instrument\": 3, \"Care\": 4, \"Bubble\": 5, \"Fat\": 6, \"Bone\": 7, \"LF\": 8, \"Dura\": 9, \"SoftTissue\": 10, \"SofrTissue\": 10, \"SoftTIssue\": 10}\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
    "class BleedingDataset(Dataset):\n",
    "    def __init__(self, image_files, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.json_paths = [f.replace('.jpeg', '.json').replace('.png', '.json') for f in self.image_paths]\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        image_path1 = self.image_paths[idx]\n",
    "        image1 = Image.open(image_path1).convert(\"RGB\")\n",
    "        image1 = image1.crop((420, 0, 1500, 1080))\n",
    "        image_path2 = self.image_paths[idx+1]\n",
    "        image2 = Image.open(image_path2).convert(\"RGB\")\n",
    "        image2 = image1.crop((420, 0, 1500, 1080))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flow = raft_model(image1.unsqueeze(0).to(device), image2.unsqueeze(0).to(device))\n",
    "            flow = flow[0].cpu().squeeze(0)  # Extract flow tensor\n",
    "        \n",
    "            flow_x = flow[0, :, :]  # x-ì„±ë¶„ (batch, H, W)\n",
    "            flow_y = flow[1, :, :]  # y-ì„±ë¶„ (batch, H, W)\n",
    "        \n",
    "            # í¬ê¸°(magnitude) ê³„ì‚°\n",
    "            magnitude = torch.sqrt(flow_x ** 2 + flow_y ** 2)\n",
    "        \n",
    "            # (3, H, W)ë¡œ ë³€í™˜\n",
    "            flow_3channel = torch.stack([flow_x, flow_y, magnitude], dim=0)\n",
    "        \n",
    "        # JSON íŒŒì¼ ë¡œë“œ\n",
    "        json_path = self.json_paths[idx]\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # ë¹ˆ ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        mask = np.zeros((data[\"imageHeight\"], data[\"imageWidth\"]), dtype=np.uint8)\n",
    "\n",
    "        # ì¶œí˜ˆ(BF) ì˜ì—­ í´ë¦¬ê³¤ ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        for shape in data[\"shapes\"]:\n",
    "            label = shape[\"label\"]\n",
    "            if label in class_map:\n",
    "                points = np.array(shape[\"points\"], dtype=np.int32)\n",
    "                cv2.fillPoly(mask, [points], class_map[label])\n",
    "\n",
    "        \n",
    "        # PIL ì´ë¯¸ì§€ ë³€í™˜ í›„ Tensor ë³€í™˜\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = mask.crop((420, 0, 1500, 1080))\n",
    "        \n",
    "        image1 = self.transform_image(image1)\n",
    "        mask = self.transform_mask(mask)\n",
    "\n",
    "        if self.augmentation:\n",
    "            gamma = random.choice(gamma_values)  # ëœë¤í•œ gamma ê°’ ì„ íƒ\n",
    "            image1 = gamma_correction(image1, gamma)\n",
    "            #mask = gamma_correction(mask, gamma)\n",
    "            #flip_type = random.choice(flip_types)\n",
    "            #image1 = random_flip(image1, random_flip)\n",
    "            #mask = random_flip(mask, random_flip)\n",
    "\n",
    "        return image1, flow_3channel, mask\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
    "class BleedingDatasetTest(Dataset):\n",
    "    def __init__(self, image_files, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        image_path1 = self.image_paths[idx]\n",
    "        image1 = Image.open(image_path1).convert(\"RGB\")\n",
    "        image1 = image1.crop((420, 0, 1500, 1080))\n",
    "        image_path2 = self.image_paths[idx+1]\n",
    "        image2 = Image.open(image_path2).convert(\"RGB\")\n",
    "        image2 = image1.crop((420, 0, 1500, 1080))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flow = raft_model(image1.unsqueeze(0).to(device), image2.unsqueeze(0).to(device))\n",
    "            flow = flow[0].cpu().squeeze(0)  # Extract flow tensor\n",
    "        \n",
    "            flow_x = flow[0, :, :]  # x-ì„±ë¶„ (batch, H, W)\n",
    "            flow_y = flow[1, :, :]  # y-ì„±ë¶„ (batch, H, W)\n",
    "        \n",
    "            # í¬ê¸°(magnitude) ê³„ì‚°\n",
    "            magnitude = torch.sqrt(flow_x ** 2 + flow_y ** 2)\n",
    "        \n",
    "            # (3, H, W)ë¡œ ë³€í™˜\n",
    "            flow_3channel = torch.stack([flow_x, flow_y, magnitude], dim=0)\n",
    "        \n",
    "        image1 = self.transform_image(image1)\n",
    "    \n",
    "        return image1, flow_3channel\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
    "class BleedingDatasetTestVideo(Dataset):\n",
    "    def __init__(self, image_files, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        image_path1 = self.image_paths[idx]\n",
    "        image1 = Image.open(image_path1).convert(\"RGB\")\n",
    "        image1 = image1.crop((840, 0, 3000, 2160))\n",
    "        image_path2 = self.image_paths[idx+1]\n",
    "        image2 = Image.open(image_path2).convert(\"RGB\")\n",
    "        image2 = image1.crop((840, 0, 3000, 2160))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            flow = raft_model(image1.unsqueeze(0).to(device), image2.unsqueeze(0).to(device))\n",
    "            flow = flow[0].cpu().squeeze(0)  # Extract flow tensor\n",
    "        \n",
    "            flow_x = flow[0, :, :]  # x-ì„±ë¶„ (batch, H, W)\n",
    "            flow_y = flow[1, :, :]  # y-ì„±ë¶„ (batch, H, W)\n",
    "        \n",
    "            # í¬ê¸°(magnitude) ê³„ì‚°\n",
    "            magnitude = torch.sqrt(flow_x ** 2 + flow_y ** 2)\n",
    "        \n",
    "            # (3, H, W)ë¡œ ë³€í™˜\n",
    "            flow_3channel = torch.stack([flow_x, flow_y, magnitude], dim=0)\n",
    "        \n",
    "        image1 = self.transform_image(image1)\n",
    "    \n",
    "        return image1, flow_3channel\n",
    "\n",
    "class CustomBackBone(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepLab_OpticalFlow, self).__init__()\n",
    "        \n",
    "        # feature extractor\n",
    "        self.rgb_conv = torchvision.models.mobilenet_v3_small(pretrained=True).features # (B, 576, H/32, W/32)\n",
    "        self.flow_conv = torchvision.models.mobilenet_v3_small(pretrained=True).features # (B, 576, H/32, W/32)\n",
    "        \n",
    "        # fusion\n",
    "        self.fusion = nn.Conv2d(576*2, 576, kernel_size=1)\n",
    "        self.out_channels = 576\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        rgb = x[:, 0:3, :, :]\n",
    "        flow = x[:, 3:6, :, :]\n",
    "        rgb_feature = self.rgb_conv(rgb)\n",
    "        flow_feature = self.flow_con(flow)\n",
    "        stack_feature = torch.cat([rgb_feature, flow_feature], dim=1)\n",
    "        return {\"out\": stack_feature}\n",
    "\n",
    "# ë°ì´í„° ë³€í™˜ ì •ì˜\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),  # ì¼ë°˜ ì´ë¯¸ì§€ìš©,\n",
    "    transforms.ToTensor(),\n",
    "    #lambda x: x.long(),\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.NEAREST),  # mask ìš©,\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x * 255,  # ë‹¤ì‹œ 255ë¥¼ ê³±í•˜ì—¬ (0,255) ë²”ìœ„ë¡œ ë³€í™˜\n",
    "    lambda x: x.long(),\n",
    "])\n",
    "\n",
    "# Dice Loss ì •ì˜\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = F.softmax(pred, dim=1)  # ì—¬ëŸ¬ í´ë˜ìŠ¤ ì˜ˆì¸¡ í™•ë¥ ë¡œ ë³€í™˜\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)  # One-hot encoding\n",
    "    intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "    union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()  # ë‹¤ì¤‘ í´ë˜ìŠ¤ dice loss\n",
    "\n",
    "def focal_loss(pred, target, gamma=2.0):\n",
    "    pred = F.softmax(pred, dim=1)  # í™•ë¥  ë¶„í¬\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)\n",
    "    ce_loss = -(target_onehot * torch.log(pred + 1e-6))  # Cross Entropy ê¸°ë°˜\n",
    "    focal_loss = (1 - pred) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "def iou_loss(pred, target, smooth=1e-6):\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)\n",
    "    intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "    union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3)) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return 1 - iou.mean()\n",
    "\n",
    "def loss_fn(pred, target):\n",
    "    return (dice_loss(pred, target) + focal_loss(pred, target) + iou_loss(pred, target)) / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed21a7c3-cd1b-472d-b3b3-93d226e5afe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '0014_spine_endoscope_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m image_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0014_spine_endoscope_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m))])  \u001b[38;5;66;03m# ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m BleedingDataset(image_files, transform\u001b[38;5;241m=\u001b[39mtransform, augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0014_spine_endoscope_data/'"
     ]
    }
   ],
   "source": [
    "image_dir = \"0014_spine_endoscope_data/\"\n",
    "image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpeg', '.png'))])  # ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# train ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\n",
    "train_dataset = BleedingDataset(image_files, transform=transform, augmentation=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34cfa73-959e-41f8-97c7-841efc2f479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4018\n",
      "Epoch [2/10], Loss: 0.3655\n",
      "Epoch [3/10], Loss: 0.3620\n",
      "Epoch [4/10], Loss: 0.3587\n",
      "Epoch [5/10], Loss: 0.3541\n",
      "Epoch [6/10], Loss: 0.3529\n",
      "Epoch [7/10], Loss: 0.3504\n",
      "Epoch [8/10], Loss: 0.3477\n",
      "Epoch [9/10], Loss: 0.3449\n",
      "Epoch [10/10], Loss: 0.3427\n",
      "ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Custom Backboneì„ ì‚¬ìš©í•œ DeepLabV3 ëª¨ë¸ ì •ì˜\n",
    "custom_backbone = CustomBackBone()\n",
    "\n",
    "# ğŸ”¹ DeepLabV3 ëª¨ë¸ì— Custom Backbone ì—°ê²°\n",
    "model = DeepLabV3(\n",
    "    backbone=custom_backbone,\n",
    "    classifier=DeepLabHead(custom_backbone.out_channels, num_classes)  # classifierì˜ ì…ë ¥ í¬ê¸°ë¥¼ backboneì— ë§ì¶¤\n",
    ")\n",
    "\n",
    "# ì¶œë ¥ ì±„ë„ ë³€ê²½ (COCO í´ë˜ìŠ¤ â†’ predict í´ë˜ìŠ¤)\n",
    "# model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"deeplabv3_bleeding_optical_flow.pth\"))\n",
    "\n",
    "# GPU ì‚¬ìš© ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, flows, masks in train_loader:\n",
    "        images, flows, masks = images.to(device), flows.to(device), masks.squeeze(1).to(device)\n",
    "        stacked_images = torch.cat([images, flows], dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)[\"out\"]  # DeepLabV3ì˜ ì¶œë ¥ ê°€ì ¸ì˜¤ê¸°\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), \"deeplabv3_bleeding_optical_flow.pth\")\n",
    "print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae1f41b-527f-4c38-b9bb-874d87e14b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:00:06.00, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 10 fps, 10 tbr, 10 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x558f081c93c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x558f081c93c0] profile High, level 2.2, 4:2:0, 8-bit\n",
      "[libx264 @ 0x558f081c93c0] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 10 fps, 10240 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x558f081c8440] video:49kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 3.130452%\n",
      "frame=   60 fps=0.0 q=-1.0 Lsize=      51kB time=00:00:05.70 bitrate=  73.0kbits/s speed=  31x    \n",
      "[libx264 @ 0x558f081c93c0] frame I:1     Avg QP: 9.56  size:  3237\n",
      "[libx264 @ 0x558f081c93c0] frame P:16    Avg QP:16.59  size:  1404\n",
      "[libx264 @ 0x558f081c93c0] frame B:43    Avg QP:22.05  size:   559\n",
      "[libx264 @ 0x558f081c93c0] consecutive B-frames:  3.3%  3.3%  0.0% 93.3%\n",
      "[libx264 @ 0x558f081c93c0] mb I  I16..4:  2.1% 93.1%  4.8%\n",
      "[libx264 @ 0x558f081c93c0] mb P  I16..4:  1.0%  3.4%  0.8%  P16..4:  8.3%  2.4%  1.2%  0.0%  0.0%    skip:82.8%\n",
      "[libx264 @ 0x558f081c93c0] mb B  I16..4:  0.1%  0.3%  0.1%  B16..8:  6.3%  1.3%  0.3%  direct: 0.9%  skip:90.7%  L0:46.2% L1:46.8% BI: 6.9%\n",
      "[libx264 @ 0x558f081c93c0] 8x8 transform intra:77.6% inter:78.4%\n",
      "[libx264 @ 0x558f081c93c0] coded y,uvDC,uvAC intra: 34.7% 52.8% 27.0% inter: 3.0% 5.1% 0.3%\n",
      "[libx264 @ 0x558f081c93c0] i16 v,h,dc,p: 11% 11%  8% 70%\n",
      "[libx264 @ 0x558f081c93c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 50% 14% 13%  3%  5%  4%  4%  4%  4%\n",
      "[libx264 @ 0x558f081c93c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 15% 27%  6%  7%  6%  6%  4%  4%\n",
      "[libx264 @ 0x558f081c93c0] i8c dc,h,v,p: 68% 12% 11%  9%\n",
      "[libx264 @ 0x558f081c93c0] Weighted P-Frames: Y:18.8% UV:18.8%\n",
      "[libx264 @ 0x558f081c93c0] ref P L0: 66.0% 10.8% 12.8%  8.7%  1.7%\n",
      "[libx264 @ 0x558f081c93c0] ref B L0: 86.0% 12.2%  1.9%\n",
      "[libx264 @ 0x558f081c93c0] ref B L1: 95.2%  4.8%\n",
      "[libx264 @ 0x558f081c93c0] kb/s:66.33\n"
     ]
    }
   ],
   "source": [
    "# video test\n",
    "\n",
    "def extract_frames(video_path, output_folder, fps=20):\n",
    "    # ë¹„ë””ì˜¤ ìº¡ì²˜ ê°ì²´ ìƒì„±\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # ì›ë³¸ ë¹„ë””ì˜¤ì˜ FPS ë° ì´ í”„ë ˆì„ ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # í”„ë ˆì„ì„ ì €ì¥í•  í´ë” ìƒì„±\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    frame_interval = video_fps // fps  # ëª‡ í”„ë ˆì„ë§ˆë‹¤ ì €ì¥í• ì§€ ê³„ì‚°\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{saved_count:05d}.png\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames and saved to {output_folder}\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "video_file = \"bleeding_test_1.mp4\"  # MP4 íŒŒì¼ ê²½ë¡œ\n",
    "video_image_dir = \"bleeding_test_3/\"  # ì €ì¥í•  í´ë”\n",
    "# extract_frames(video_file, video_image_dir)\n",
    "\n",
    "output_dir = \"bleeding_test_result_3/\"\n",
    "\n",
    "video_image_files = sorted([os.path.join(video_image_dir, f) for f in os.listdir(video_image_dir) if f.endswith(('.jpeg', '.png'))])  # ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "# test ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±\n",
    "video_test_dataset = BleedingDatasetTestVideo(video_image_files, transform=transform, augmentation=False)\n",
    "video_test_dataloader = DataLoader(video_test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "# ğŸ”¹ Custom Backboneì„ ì‚¬ìš©í•œ DeepLabV3 ëª¨ë¸ ì •ì˜\n",
    "custom_backbone = CustomBackBone()\n",
    "\n",
    "# ğŸ”¹ DeepLabV3 ëª¨ë¸ì— Custom Backbone ì—°ê²°\n",
    "model = DeepLabV3(\n",
    "    backbone=custom_backbone,\n",
    "    classifier=DeepLabHead(custom_backbone.out_channels, num_classes)  # classifierì˜ ì…ë ¥ í¬ê¸°ë¥¼ backboneì— ë§ì¶¤\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"deeplabv3_bleeding_optical_flow.pth\"))\n",
    "\n",
    "# GPU ì‚¬ìš© ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "total_num = 0\n",
    "for images in video_test_dataloader:\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = model(images)[\"out\"]  # DeepLabV3ì˜ ì¶œë ¥ ê°€ì ¸ì˜¤ê¸°\n",
    "        \n",
    "    for image, pred in zip(images, preds):\n",
    "        total_num += 1\n",
    "        \n",
    "        # ğŸ“Œ í›„ì²˜ë¦¬ (Sigmoid + Threshold)\n",
    "        pred_mask = torch.argmax(pred, dim=0).cpu().numpy()  # (512, 512)\n",
    "        \n",
    "        # ì›ë³¸ ì´ë¯¸ì§€, ë§ˆìŠ¤í¬ ë³€í™˜\n",
    "        original_image = image.cpu().numpy().transpose(1,2,0)\n",
    "        original_image = (original_image * 255).astype(np.uint8)  # ì •ê·œí™” í•´ì œ\n",
    "       \n",
    "        # âœ… ì»¬ëŸ¬ë§µ ì ìš© (GT = Green, Pred = Red, Overlap = Yellow)\n",
    "        overlay = np.array(original_image)\n",
    "        overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        green = [0, 255, 0]  # Ground Truth (GT) - Green\n",
    "        red = [0, 0, 255]  # Prediction - Red\n",
    "        yellow = [0, 255, 255]  # Overlapping - Yellow\n",
    "\n",
    "        mask_layer = np.zeros_like(overlay, dtype=np.uint8)\n",
    "        mask_layer[pred_mask == 1] = green  # Prediction\n",
    "\n",
    "        # âœ… ìµœì¢… í•©ì„±\n",
    "        blended = cv2.addWeighted(overlay, 0.7, mask_layer, 0.5, 0)\n",
    "\n",
    "        # blended = cv2.resize(blended, (1920, 1080), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "        # âœ… ì €ì¥\n",
    "        filename = f\"output_{total_num:04d}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, blended)\n",
    "\n",
    "os.system(f\"cd {output_dir} && ffmpeg -framerate 20 -i output_%04d.png -c:v libx264 -pix_fmt yuv420p output.mp4 & cd ..\")\n",
    "print(\"video test completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac861d73-357d-4fc5-a491-91cc7d6c2b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
