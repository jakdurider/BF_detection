{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fac117-6c69-4c60-826c-18bd02c2e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/bf_optical_flow/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/bf_optical_flow/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!!alt_cuda_corr is not compiled!!]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation import DeepLabV3\n",
    "from torchvision.models.optical_flow import raft_small\n",
    "import datetime\n",
    "import ptlflow\n",
    "from ptlflow.utils import flow_utils\n",
    "from ptlflow.utils.io_adapter import IOAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "90f90a52-8da8-4f99-bcf0-ed989f5441e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-25 12:31:06.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mptlflow\u001b[0m:\u001b[36mrestore_model\u001b[0m:\u001b[36m280\u001b[0m - \u001b[1mRestored model state from checkpoint: things\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "'''\n",
    "raft_model = raft_small(pretrained=True, progress=False).to(device)\n",
    "raft_model = raft_model.eval()\n",
    "'''\n",
    "flow_model = ptlflow.get_model('fastflownet', ckpt_path='things')\n",
    "\n",
    "gamma_values = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8]  # Gamma values\n",
    "rotate_values = [-90, 0, 90, 180]\n",
    "def transform(img):\n",
    "    return img\n",
    "\n",
    "def gamma_correction(img, gamma):\n",
    "    return TF.adjust_gamma(img, gamma)\n",
    "\n",
    "flip_types = [\"horizontal\", \"vertical\", \"both\", \"none\"]\n",
    "\n",
    "def random_flip(img, flip_type):\n",
    "    if flip_type == \"horizontal\":\n",
    "        return TF.hflip(img)\n",
    "    elif flip_type == \"vertical\":\n",
    "        return TF.vflip(img)\n",
    "    elif flip_type == \"both\":\n",
    "        return TF.hflip(TF.vflip(img))\n",
    "    return img  # No flip\n",
    "\n",
    "def correct_affine(prev_frame, curr_frame):\n",
    "    \"\"\"\n",
    "    특징점을 기반으로 Affine Transform을 사용하여 시점 보정 (이동 + 회전 포함)\n",
    "    \"\"\"\n",
    "    orb = cv2.ORB_create()  # ORB 특징점 검출기 사용\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 특징점 검출\n",
    "    kp1, des1 = orb.detectAndCompute(prev_gray, None)\n",
    "    kp2, des2 = orb.detectAndCompute(curr_gray, None)\n",
    "\n",
    "    # 특징점이 검출되지 않았을 경우 대비\n",
    "    if des1 is None or des2 is None:\n",
    "        return curr_frame  # 원본 프레임 반환 (보정 없이 유지)\n",
    "\n",
    "    # 특징점 매칭 (BFMatcher 사용)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # 매칭이 너무 적으면 보정 불가능\n",
    "    if len(matches) < 10:\n",
    "        return curr_frame  # 원본 프레임 반환\n",
    "\n",
    "    # 좋은 매칭 선택\n",
    "    matches = sorted(matches, key=lambda x: x.distance)[:50]\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Affine Transform Matrix 계산\n",
    "    M, _ = cv2.estimateAffine2D(dst_pts, src_pts)\n",
    "\n",
    "    # 변환 행렬이 None이면 원본 반환\n",
    "    if M is None:\n",
    "        return curr_frame\n",
    "\n",
    "    # Affine 변환 적용\n",
    "    h, w = prev_frame.shape[:2]\n",
    "    corrected_frame = cv2.warpAffine(curr_frame, M, (w, h))\n",
    "\n",
    "    return corrected_frame\n",
    "\n",
    "# {'Instrument', 'Care', 'Bubble', 'unkown', 'unknown', 'Fat', 'SoftTIssue', 'Dura', 'BF', 'SoftTissue', 'vessel', 'Vessel', 'Bone', 'LF', 'SofrTissue'}\n",
    "num_classes = 11 # Background, BF, Vessel, Instrument, Care, Bubble, Fat, Bone, LF, Dura, SoftTissue\n",
    "\n",
    "# 클래스별 라벨 매핑\n",
    "class_map = {\"BF\": 1, \"Vessel\": 9, \"vessel\": 9, \"Instrument\": 3, \"Care\": 4, \"Bubble\": 5, \"Fat\": 6, \"Bone\": 7, \"LF\": 8, \"Dura\": 2, \"SoftTissue\": 10, \"SofrTissue\": 10, \"SoftTIssue\": 10}\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BleedingDataset(Dataset):\n",
    "    def __init__(self, image_files, x_offset=420, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.json_paths = [f.replace('.jpeg', '.json').replace('.png', '.json') for f in self.image_paths]\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.image1 = None\n",
    "        self.image2 = None\n",
    "        self.image1_resized = None\n",
    "        self.image2_resized = None\n",
    "        self.x_offset = x_offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_offset = self.x_offset\n",
    "        # 원본 이미지 로드\n",
    "        if idx == 0:\n",
    "            image_path1 = self.image_paths[idx]\n",
    "            \n",
    "            # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "            file_bytes1 = np.fromfile(image_path1, dtype=np.uint8)  \n",
    "            image1 = cv2.imdecode(file_bytes1, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "            # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "            image1 = image1[0:1080, x_offset:x_offset + 1080]  # crop(x1, y1, x2, y2)\n",
    "            image1_resized = cv2.resize(image1, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "            self.image1_resized = image1_resized\n",
    "            image1 = torch.from_numpy(image1_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "            self.image1 = image1\n",
    "\n",
    "\n",
    "            image_path2 = self.image_paths[idx+1]\n",
    "            \n",
    "            # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "            file_bytes2 = np.fromfile(image_path2, dtype=np.uint8)  \n",
    "            image2 = cv2.imdecode(file_bytes2, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "            # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "            image2 = image2[0:1080, x_offset:x_offset +1080]  # crop(x1, y1, x2, y2)\n",
    "            image2_resized = cv2.resize(image2, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "            self.image2_resized = image2_resized\n",
    "            image2 = torch.from_numpy(image2_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "            self.image2 = image2\n",
    "\n",
    "        else:\n",
    "            self.image1 = self.image2\n",
    "            image1 = self.image1\n",
    "            self.image2 = self.image3\n",
    "            image2 = self.image2\n",
    "\n",
    "        \n",
    "        image_path3 = self.image_paths[idx+2]\n",
    "            \n",
    "        # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "        file_bytes3 = np.fromfile(image_path3, dtype=np.uint8)  \n",
    "        image3 = cv2.imdecode(file_bytes3, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "        image3 = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "        # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "        image3 = image3[0:1080, x_offset:x_offset+1080]  # crop(x1, y1, x2, y2)\n",
    "        image3_resized = cv2.resize(image3, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "        self.image3_resized = image3_resized\n",
    "        image3 = torch.from_numpy(image3_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "        self.image3 = image3\n",
    "        \n",
    "        # JSON 파일 로드\n",
    "        json_path = self.json_paths[idx+1]\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # 빈 마스크 생성\n",
    "        mask = np.zeros((data[\"imageHeight\"], data[\"imageWidth\"]), dtype=np.uint8)\n",
    "\n",
    "        # 출혈(BF) 영역 폴리곤 마스크 생성\n",
    "        for shape in data[\"shapes\"]:\n",
    "            label = shape[\"label\"]\n",
    "            if label in class_map:\n",
    "                points = np.array(shape[\"points\"], dtype=np.int32)\n",
    "                cv2.fillPoly(mask, [points], class_map[label])\n",
    "\n",
    "        \n",
    "        # PIL 이미지 변환 후 Tensor 변환\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = mask.crop((x_offset, 0, x_offset+1080, 1080))\n",
    "        mask = self.transform_mask(mask)\n",
    "\n",
    "        # if self.augmentation:\n",
    "        gamma = random.choice(gamma_values)\n",
    "        image1_gamma = gamma_correction(image1, gamma)\n",
    "        image2_gamma = gamma_correction(image2, gamma)\n",
    "        image3_gamma = gamma_correction(image3, gamma)\n",
    "\n",
    "        rotate_degree = random.choice(rotate_values)\n",
    "        image1_rotate = TF.rotate(image1_gamma, angle=rotate_degree)\n",
    "        image2_rotate = TF.rotate(image2_gamma, angle=rotate_degree)\n",
    "        image3_rotate = TF.rotate(image3_gamma, angle=rotate_degree)\n",
    "\n",
    "        seq = torch.stack([image1_rotate, image2_rotate, image3_rotate], dim=0)\n",
    "        \n",
    "        mask = TF.rotate(mask, angle=rotate_degree)\n",
    "            \n",
    "        return seq, mask\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BleedingDatasetTest(Dataset):\n",
    "    def __init__(self, image_files, x_offset=0, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        # self.yolo_image_paths = yolo_image_files\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.image1 = None\n",
    "        self.image2 = None\n",
    "        self.yolo_image1 = None\n",
    "        self.yolo_image2 = None\n",
    "        self.image1_resized = None\n",
    "        self.image2_resized = None\n",
    "        self.x_offset = x_offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_offset = self.x_offset\n",
    "        if idx == 0:\n",
    "            image_path1 = self.image_paths[idx]\n",
    "            \n",
    "            # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "            file_bytes1 = np.fromfile(image_path1, dtype=np.uint8)  \n",
    "            image1 = cv2.imdecode(file_bytes1, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "            # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "            image1 = image1[0:1080, x_offset:x_offset + 1080]  # crop(x1, y1, x2, y2)\n",
    "            image1_resized = cv2.resize(image1, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "            self.image1_resized = image1_resized\n",
    "            image1 = torch.from_numpy(image1_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "            self.image1 = image1\n",
    "\n",
    "\n",
    "            image_path2 = self.image_paths[idx+1]\n",
    "            \n",
    "            # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "            file_bytes2 = np.fromfile(image_path2, dtype=np.uint8)  \n",
    "            image2 = cv2.imdecode(file_bytes2, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "            # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "            image2 = image2[0:1080, x_offset:x_offset + 1080]  # crop(x1, y1, x2, y2)\n",
    "            image2_resized = cv2.resize(image2, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "            self.image2_resized = image2_resized\n",
    "            image2 = torch.from_numpy(image2_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "            self.image2 = image2\n",
    "\n",
    "        else:\n",
    "            self.image1 = self.image2\n",
    "            image1 = self.image1\n",
    "            self.image2 = self.image3\n",
    "            image2 = self.image2\n",
    "\n",
    "        \n",
    "        image_path3 = self.image_paths[idx+2]\n",
    "            \n",
    "        # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "        file_bytes3 = np.fromfile(image_path3, dtype=np.uint8)  \n",
    "        image3 = cv2.imdecode(file_bytes3, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "        image3 = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "        # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "        image3 = image3[0:1080, x_offset:x_offset + 1080]  # crop(x1, y1, x2, y2)\n",
    "        image3_resized = cv2.resize(image3, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "        self.image3_resized = image3_resized\n",
    "        image3 = torch.from_numpy(image3_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "        self.image3 = image3\n",
    "\n",
    "        seq = torch.stack([image1, image2, image3], dim=0)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BleedingDatasetTestVideo(Dataset):\n",
    "    def __init__(self, image_files, x_offset=0, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.image1 = None\n",
    "        self.image2 = None\n",
    "        self.image1_resized = None\n",
    "        self.image2_resized = None\n",
    "        self.x_offset = x_offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 원본 이미지 로드\n",
    "        x_offset = self.x_offset\n",
    "        if idx == 0:\n",
    "            image_path1 = self.image_paths[idx]\n",
    "            \n",
    "            # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "            file_bytes1 = np.fromfile(image_path1, dtype=np.uint8)  \n",
    "            image1 = cv2.imdecode(file_bytes1, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "            # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "            image1 = image1[0:2160, x_offset:x_offset + 2160]  # crop(x1, y1, x2, y2)\n",
    "            image1_resized = cv2.resize(image1, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "            self.image1_resized = image1_resized\n",
    "            image1 = torch.from_numpy(image1_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "            self.image1 = image1\n",
    "\n",
    "\n",
    "            image_path2 = self.image_paths[idx+1]\n",
    "            \n",
    "            # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "            file_bytes2 = np.fromfile(image_path2, dtype=np.uint8)  \n",
    "            image2 = cv2.imdecode(file_bytes2, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "            # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "            image2 = image2[0:2160, x_offset:x_offset + 2160]  # crop(x1, y1, x2, y2)\n",
    "            image2_resized = cv2.resize(image2, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "            self.image2_resized = image2_resized\n",
    "            image2 = torch.from_numpy(image2_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "            self.image2 = image2\n",
    "\n",
    "        else:\n",
    "            self.image1 = self.image2\n",
    "            image1 = self.image1\n",
    "            self.image2 = self.image3\n",
    "            image2 = self.image2\n",
    "\n",
    "        \n",
    "        image_path3 = self.image_paths[idx+2]\n",
    "            \n",
    "        # 🚀 빠른 파일 로딩: np.fromfile() + cv2.imdecode()\n",
    "        file_bytes3 = np.fromfile(image_path3, dtype=np.uint8)  \n",
    "        image3 = cv2.imdecode(file_bytes3, cv2.IMREAD_COLOR)  # BGR로 로드\n",
    "        image3 = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)  # RGB 변환\n",
    "\n",
    "        # 🚀 빠른 크롭 (Pillow보다 OpenCV가 빠름)\n",
    "        image3 = image3[0:2160, x_offset:x_offset + 2160]  # crop(x1, y1, x2, y2)\n",
    "        image3_resized = cv2.resize(image3, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "        self.image3_resized = image3_resized\n",
    "        image3 = torch.from_numpy(image3_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "        self.image3 = image3\n",
    "\n",
    "        seq = torch.stack([image1, image2, image3], dim=0)\n",
    "        \n",
    "        return seq\n",
    "\n",
    "class TemporalSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels=3, heads=3):\n",
    "        super(TemporalSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (in_channels // heads) ** 0.5\n",
    "        \n",
    "        self.qkv_proj = nn.Conv3d(in_channels, in_channels * 3, kernel_size=1)\n",
    "        self.output_proj = nn.Conv3d(in_channels, in_channels, kernel_size=1)\n",
    "        \n",
    "        # 게이트 학습 모듈 추가\n",
    "        self.gate_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels // 2, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels // 2, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        입력: x [B, C, T, H, W]  \n",
    "        출력: [B, C, H, W] — 시간 축 요약된 feature\n",
    "        \"\"\"\n",
    "        B, C, T, H, W = x.shape\n",
    "        qkv = self.qkv_proj(x)  # [B, 3*C, T, H, W]\n",
    "        qkv = qkv.view(B, 3, self.heads, C // self.heads, T, H, W)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]  # 각각 [B, heads, C//heads, T, H, W]\n",
    "\n",
    "        # Attention score 계산\n",
    "        attn_scores = (q * k).sum(dim=2) / self.scale  # [B, heads, T, H, W]\n",
    "        attn_weights = F.softmax(attn_scores, dim=2)  # 시간축(T)에 대해 softmax\n",
    "\n",
    "        '''\n",
    "        # Value에 attention weight 곱해서 시간축 요약\n",
    "        attn_output = (attn_weights.unsqueeze(2) * v).sum(dim=3)  # [B, heads, C//heads, H, W]\n",
    "\n",
    "        # heads 합치기\n",
    "        out = attn_output.reshape(B, C, H, W)\n",
    "        out = self.output_proj(out.unsqueeze(2)).squeeze(2)  # Conv3D→2D처럼 사용\n",
    "\n",
    "        return out\n",
    "        '''\n",
    "        weighted_v = (attn_weights.unsqueeze(2) * v).sum(dim=3)  # [B, heads, C//heads, H, W]\n",
    "        out = weighted_v.reshape(B, C, H, W).unsqueeze(2)\n",
    "\n",
    "        out = self.output_proj(out)  # [B, C, 1, H, W]\n",
    "\n",
    "        # 🎯 추가된 게이트로 시간정보의 신뢰도 조절\n",
    "        gate = self.gate_conv(x)  # [B, 1, T, H, W]\n",
    "        gate_score = gate[:, :, 1]  # 현재 프레임 위치 gate만 사용 (T=3에서 가운데)\n",
    "\n",
    "        gated_out = out.squeeze(2) * gate_score  # 게이트 적용 후 output 반환\n",
    "\n",
    "        return gated_out\n",
    "        \n",
    "class CustomBackBone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomBackBone, self).__init__()\n",
    "        \n",
    "        # feature extractor\n",
    "        self.rgb_conv = torchvision.models.mobilenet_v3_small(pretrained=True).features # (B, 576, H/32, W/32)\n",
    "        self.flow_conv = torchvision.models.mobilenet_v3_small(pretrained=True).features # (B, 576, H/32, W/32)\n",
    "        self.temporal_attention = TemporalSelfAttention(in_channels=3, heads=3)\n",
    "        \n",
    "        # fusion\n",
    "        self.fusion = nn.Conv2d(576*2, 576, kernel_size=1)\n",
    "        self.out_channels = 576\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_seq = x\n",
    "        # x_seq: [B, T(3), C, H, W] → input은 연속된 RGB 프레임\n",
    "        B, T, C, H, W = x_seq.shape\n",
    "        rgb_feature = self.rgb_conv(x_seq[:, 1, :, :, :])\n",
    "        x_seq = x_seq.permute(0, 2, 1, 3, 4)  # → [B, C, T, H, W]\n",
    "        x_attended = self.temporal_attention(x_seq)  # → [B, C, H, W]\n",
    "        flow_feature = self.flow_conv(x_attended)  # → [B, 576, H/32, W/32]\n",
    "        stack_feature = self.fusion(torch.cat([rgb_feature, flow_feature], dim=1))\n",
    "        \n",
    "        return {\"out\": stack_feature + rgb_feature}\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),  # 일반 이미지용,\n",
    "    transforms.ToTensor(),\n",
    "    #lambda x: x.long(),\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.NEAREST),  # mask 용,\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x * 255,  # 다시 255를 곱하여 (0,255) 범위로 변환\n",
    "    lambda x: x.long(),\n",
    "])\n",
    "\n",
    "def apply_transform_cv2(image_np):\n",
    "    \"\"\"OpenCV를 이용하여 numpy.ndarray를 변환\"\"\"\n",
    "    image_resized = cv2.resize(image_np, (512, 512), interpolation=cv2.INTER_LINEAR)  # BILINEAR 보간\n",
    "    image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float() / 255.0  # (H, W, C) → (C, H, W)\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "# Dice Loss 정의\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = F.softmax(pred, dim=1)  # 여러 클래스 예측 확률로 변환\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)  # One-hot encoding\n",
    "    intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "    union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()  # 다중 클래스 dice loss\n",
    "\n",
    "def focal_loss(pred, target, gamma=2.0):\n",
    "    alpha = [1.0, 0.3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "    pred = F.softmax(pred, dim=1)  # 확률 분포\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "    pt = (pred * target_onehot).sum(dim=1)\n",
    "    log_pt = torch.log(pt + 1e-6)  # log(0) 방지\n",
    "    # Focal Loss 적용\n",
    "    focal_loss = -((1 - pt) ** gamma) * log_pt\n",
    "    return focal_loss.mean()\n",
    "\n",
    "    '''\n",
    "    ce_loss = -(target_onehot * torch.log(pred + 1e-6))  # Cross Entropy 기반\n",
    "    focal_loss = (1 - pred) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "    '''\n",
    "\n",
    "def iou_loss(pred, target, smooth=1e-6):\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)\n",
    "    intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "    union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3)) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return 1 - iou.mean()\n",
    "\n",
    "def fp_penalty_loss(pred, target):\n",
    "    pred_soft = F.softmax(pred, dim=1)\n",
    "    pred_label = pred_soft.argmax(dim=1)  # [B, H, W]\n",
    "    fp_mask = (pred_label != target) & (pred_label != 0) & (target == 0)\n",
    "\n",
    "    fp_confidence = pred_soft.max(dim=1)[0]  # confidence score\n",
    "    penalty = (fp_mask * fp_confidence).mean()\n",
    "    return penalty\n",
    "\n",
    "def loss_fn(pred, target):\n",
    "    return ((dice_loss(pred, target) + focal_loss(pred, target) + iou_loss(pred, target)) / 3) + 0.5 * fp_penalty_loss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ed21a7c3-cd1b-472d-b3b3-93d226e5afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"0014_spine_endoscope_data/\"\n",
    "image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpeg', '.png', '.jpg'))])  # 이미지 파일 리스트\n",
    "\n",
    "# train 데이터셋 및 DataLoader 생성\n",
    "train_dataset = BleedingDataset(image_files, transform=transform, augmentation=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b34cfa73-959e-41f8-97c7-841efc2f479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9095\n",
      "Epoch [2/100], Loss: 0.9008\n",
      "Epoch [3/100], Loss: 0.8864\n",
      "Epoch [4/100], Loss: 0.8754\n",
      "Epoch [5/100], Loss: 0.8809\n",
      "Epoch [6/100], Loss: 0.8712\n",
      "Epoch [7/100], Loss: 0.8557\n",
      "Epoch [8/100], Loss: 0.8502\n",
      "Epoch [9/100], Loss: 0.8342\n",
      "Epoch [10/100], Loss: 0.8216\n",
      "Epoch [11/100], Loss: 0.8181\n",
      "Epoch [12/100], Loss: 0.8032\n",
      "Epoch [13/100], Loss: 0.8120\n",
      "Epoch [14/100], Loss: 0.8014\n",
      "Epoch [15/100], Loss: 0.7917\n",
      "Epoch [16/100], Loss: 0.7841\n",
      "Epoch [17/100], Loss: 0.7706\n",
      "Epoch [18/100], Loss: 0.7764\n",
      "Epoch [19/100], Loss: 0.7600\n",
      "Epoch [20/100], Loss: 0.7655\n",
      "Epoch [21/100], Loss: 0.7484\n",
      "Epoch [22/100], Loss: 0.7380\n",
      "Epoch [23/100], Loss: 0.7461\n",
      "Epoch [24/100], Loss: 0.7453\n",
      "Epoch [25/100], Loss: 0.7283\n",
      "Epoch [26/100], Loss: 0.7140\n",
      "Epoch [27/100], Loss: 0.7102\n",
      "Epoch [28/100], Loss: 0.6952\n",
      "Epoch [29/100], Loss: 0.6968\n",
      "Epoch [30/100], Loss: 0.6847\n",
      "Epoch [31/100], Loss: 0.6732\n",
      "Epoch [32/100], Loss: 0.6651\n",
      "Epoch [33/100], Loss: 0.6593\n",
      "Epoch [34/100], Loss: 0.6480\n",
      "Epoch [35/100], Loss: 0.6490\n",
      "Epoch [36/100], Loss: 0.6363\n",
      "Epoch [37/100], Loss: 0.6373\n",
      "Epoch [38/100], Loss: 0.6245\n",
      "Epoch [39/100], Loss: 0.6226\n",
      "Epoch [40/100], Loss: 0.6227\n",
      "Epoch [41/100], Loss: 0.6131\n",
      "Epoch [42/100], Loss: 0.6016\n",
      "Epoch [43/100], Loss: 0.5961\n",
      "Epoch [44/100], Loss: 0.5929\n",
      "Epoch [45/100], Loss: 0.5866\n",
      "Epoch [46/100], Loss: 0.5838\n",
      "Epoch [47/100], Loss: 0.5828\n",
      "Epoch [48/100], Loss: 0.5798\n",
      "Epoch [49/100], Loss: 0.5720\n",
      "Epoch [50/100], Loss: 0.5727\n",
      "Epoch [51/100], Loss: 0.5657\n",
      "Epoch [52/100], Loss: 0.5669\n",
      "Epoch [53/100], Loss: 0.5611\n",
      "Epoch [54/100], Loss: 0.5564\n",
      "Epoch [55/100], Loss: 0.5542\n",
      "Epoch [56/100], Loss: 0.5514\n",
      "Epoch [57/100], Loss: 0.5460\n",
      "Epoch [58/100], Loss: 0.5455\n",
      "Epoch [59/100], Loss: 0.5446\n",
      "Epoch [60/100], Loss: 0.5422\n",
      "Epoch [61/100], Loss: 0.5378\n",
      "Epoch [62/100], Loss: 0.5417\n",
      "Epoch [63/100], Loss: 0.5732\n",
      "Epoch [64/100], Loss: 0.5732\n",
      "Epoch [65/100], Loss: 0.5508\n",
      "Epoch [66/100], Loss: 0.5470\n",
      "Epoch [67/100], Loss: 0.5332\n",
      "Epoch [68/100], Loss: 0.5289\n",
      "Epoch [69/100], Loss: 0.5270\n",
      "Epoch [70/100], Loss: 0.5218\n",
      "Epoch [71/100], Loss: 0.5180\n",
      "Epoch [72/100], Loss: 0.5164\n",
      "Epoch [73/100], Loss: 0.5154\n",
      "Epoch [74/100], Loss: 0.5144\n",
      "Epoch [75/100], Loss: 0.5134\n",
      "Epoch [76/100], Loss: 0.5109\n",
      "Epoch [77/100], Loss: 0.5097\n",
      "Epoch [78/100], Loss: 0.5266\n",
      "Epoch [79/100], Loss: 0.6429\n",
      "Epoch [80/100], Loss: 0.5673\n",
      "Epoch [81/100], Loss: 0.5277\n",
      "Epoch [82/100], Loss: 0.5152\n",
      "Epoch [83/100], Loss: 0.5078\n",
      "Epoch [84/100], Loss: 0.5036\n",
      "Epoch [85/100], Loss: 0.4999\n",
      "Epoch [86/100], Loss: 0.4994\n",
      "Epoch [87/100], Loss: 0.5008\n",
      "Epoch [88/100], Loss: 0.4996\n",
      "Epoch [89/100], Loss: 0.5005\n",
      "Epoch [90/100], Loss: 0.4995\n",
      "Epoch [91/100], Loss: 0.4997\n",
      "Epoch [92/100], Loss: 0.4992\n",
      "Epoch [93/100], Loss: 0.4975\n",
      "Epoch [94/100], Loss: 0.4941\n",
      "Epoch [95/100], Loss: 0.4922\n",
      "Epoch [96/100], Loss: 0.4912\n",
      "Epoch [97/100], Loss: 0.4941\n",
      "Epoch [98/100], Loss: 0.4969\n",
      "Epoch [99/100], Loss: 0.4926\n",
      "Epoch [100/100], Loss: 0.4906\n",
      "모델 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Custom Backbone을 사용한 DeepLabV3 모델 정의\n",
    "custom_backbone = CustomBackBone()\n",
    "\n",
    "# 🔹 DeepLabV3 모델에 Custom Backbone 연결\n",
    "model = DeepLabV3(\n",
    "    backbone=custom_backbone,\n",
    "    classifier=DeepLabHead(custom_backbone.out_channels, num_classes)  # classifier의 입력 크기를 backbone에 맞춤\n",
    ")\n",
    "\n",
    "# 출력 채널 변경 (COCO 클래스 → predict 클래스)\n",
    "# model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "# model.load_state_dict(torch.load(\"deeplabv3_bleeding_self_attention_best.pth\"))\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        if len(images) <= 1: # batch number\n",
    "            continue\n",
    "        images, masks = images.to(device), masks.squeeze(1).to(device)\n",
    "        # stacked_images = torch.cat(images, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)[\"out\"]  # DeepLabV3의 출력 가져오기\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), \"deeplabv3_bleeding_self_attention_new.pth\")\n",
    "print(\"모델 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fae1f41b-527f-4c38-b9bb-874d87e14b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num: 208\n",
      "total time(ms):0:00:18.540117\n",
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:00:07.88, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x56497dce9e80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x56497dce9e80] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x56497dce9e80] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 25 fps, 12800 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x56497dce8f00] video:506kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.601443%\n",
      "frame=  197 fps=182 q=-1.0 Lsize=     509kB time=00:00:07.76 bitrate= 537.2kbits/s speed=7.15x    \n",
      "[libx264 @ 0x56497dce9e80] frame I:2     Avg QP:21.82  size:  6606\n",
      "[libx264 @ 0x56497dce9e80] frame P:61    Avg QP:23.98  size:  3824\n",
      "[libx264 @ 0x56497dce9e80] frame B:134   Avg QP:24.86  size:  2020\n",
      "[libx264 @ 0x56497dce9e80] consecutive B-frames:  7.1%  3.0% 10.7% 79.2%\n",
      "[libx264 @ 0x56497dce9e80] mb I  I16..4: 23.4% 69.4%  7.2%\n",
      "[libx264 @ 0x56497dce9e80] mb P  I16..4: 18.7% 30.6%  0.8%  P16..4: 30.7%  6.3%  1.7%  0.0%  0.0%    skip:11.3%\n",
      "[libx264 @ 0x56497dce9e80] mb B  I16..4:  4.1%  5.4%  0.1%  B16..8: 36.2%  4.7%  0.4%  direct: 3.6%  skip:45.5%  L0:44.9% L1:51.7% BI: 3.3%\n",
      "[libx264 @ 0x56497dce9e80] 8x8 transform intra:60.1% inter:95.1%\n",
      "[libx264 @ 0x56497dce9e80] coded y,uvDC,uvAC intra: 29.1% 59.5% 3.4% inter: 13.7% 27.3% 0.2%\n",
      "[libx264 @ 0x56497dce9e80] i16 v,h,dc,p: 11% 16%  3% 70%\n",
      "[libx264 @ 0x56497dce9e80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 17% 18%  9%  7%  6%  6%  8%  5%\n",
      "[libx264 @ 0x56497dce9e80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 22% 16%  6%  8%  7%  5%  5%  2%\n",
      "[libx264 @ 0x56497dce9e80] i8c dc,h,v,p: 51% 18% 24%  7%\n",
      "[libx264 @ 0x56497dce9e80] Weighted P-Frames: Y:29.5% UV:21.3%\n",
      "[libx264 @ 0x56497dce9e80] ref P L0: 61.2%  8.8% 19.8%  8.6%  1.6%\n",
      "[libx264 @ 0x56497dce9e80] ref B L0: 82.3% 13.9%  3.8%\n",
      "[libx264 @ 0x56497dce9e80] ref B L1: 93.9%  6.1%\n",
      "[libx264 @ 0x56497dce9e80] kb/s:525.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num: 208\n",
      "total time(ms):0:00:12.164772\n",
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output.mp4': No such file or directory\n",
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:00:07.96, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x555c9fe52e80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x555c9fe52e80] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x555c9fe52e80] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 25 fps, 12800 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x555c9fe51f00] video:424kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.675596%\n",
      "frame=  199 fps=0.0 q=-1.0 Lsize=     427kB time=00:00:07.84 bitrate= 446.3kbits/s speed= 8.8x    \n",
      "[libx264 @ 0x555c9fe52e80] frame I:3     Avg QP:21.16  size:  5422\n",
      "[libx264 @ 0x555c9fe52e80] frame P:93    Avg QP:23.43  size:  2761\n",
      "[libx264 @ 0x555c9fe52e80] frame B:103   Avg QP:23.85  size:  1560\n",
      "[libx264 @ 0x555c9fe52e80] consecutive B-frames: 24.1% 17.1% 10.6% 48.2%\n",
      "[libx264 @ 0x555c9fe52e80] mb I  I16..4: 36.2% 59.0%  4.8%\n",
      "[libx264 @ 0x555c9fe52e80] mb P  I16..4: 26.3% 15.8%  0.9%  P16..4: 37.8%  4.0%  1.3%  0.0%  0.0%    skip:13.9%\n",
      "[libx264 @ 0x555c9fe52e80] mb B  I16..4:  3.7%  2.1%  0.1%  B16..8: 35.2%  2.5%  0.2%  direct: 8.6%  skip:47.5%  L0:46.6% L1:50.8% BI: 2.6%\n",
      "[libx264 @ 0x555c9fe52e80] 8x8 transform intra:37.9% inter:97.0%\n",
      "[libx264 @ 0x555c9fe52e80] coded y,uvDC,uvAC intra: 13.4% 49.2% 3.6% inter: 9.8% 36.9% 0.5%\n",
      "[libx264 @ 0x555c9fe52e80] i16 v,h,dc,p: 19% 16%  3% 62%\n",
      "[libx264 @ 0x555c9fe52e80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32% 15% 27%  6%  4%  4%  4%  6%  2%\n",
      "[libx264 @ 0x555c9fe52e80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 28% 25%  3%  3%  3%  2%  2%  2%\n",
      "[libx264 @ 0x555c9fe52e80] i8c dc,h,v,p: 50% 28% 18%  5%\n",
      "[libx264 @ 0x555c9fe52e80] Weighted P-Frames: Y:38.7% UV:33.3%\n",
      "[libx264 @ 0x555c9fe52e80] ref P L0: 60.4%  5.5% 19.8%  9.2%  5.1%\n",
      "[libx264 @ 0x555c9fe52e80] ref B L0: 76.4% 19.4%  4.3%\n",
      "[libx264 @ 0x555c9fe52e80] ref B L1: 95.3%  4.7%\n",
      "[libx264 @ 0x555c9fe52e80] kb/s:435.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num: 736\n",
      "total time(ms):0:03:24.493396\n",
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output.mp4': No such file or directory\n",
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:00:29.04, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55a7921a0e80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55a7921a0e80] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55a7921a0e80] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 25 fps, 12800 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x55a79219ff00] video:2255kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.414160%\n",
      "frame=  726 fps=148 q=-1.0 Lsize=    2264kB time=00:00:28.92 bitrate= 641.4kbits/s speed=5.88x    \n",
      "[libx264 @ 0x55a7921a0e80] frame I:3     Avg QP:21.87  size: 11648\n",
      "[libx264 @ 0x55a7921a0e80] frame P:188   Avg QP:23.67  size:  5404\n",
      "[libx264 @ 0x55a7921a0e80] frame B:535   Avg QP:25.23  size:  2350\n",
      "[libx264 @ 0x55a7921a0e80] consecutive B-frames:  0.8%  1.9%  2.5% 94.8%\n",
      "[libx264 @ 0x55a7921a0e80] mb I  I16..4: 17.2% 65.4% 17.4%\n",
      "[libx264 @ 0x55a7921a0e80] mb P  I16..4:  9.3% 16.6%  1.7%  P16..4: 43.9%  9.9%  5.2%  0.0%  0.0%    skip:13.4%\n",
      "[libx264 @ 0x55a7921a0e80] mb B  I16..4:  2.2%  2.8%  0.3%  B16..8: 39.1%  5.0%  0.9%  direct: 5.1%  skip:44.7%  L0:45.4% L1:50.9% BI: 3.7%\n",
      "[libx264 @ 0x55a7921a0e80] 8x8 transform intra:57.9% inter:83.2%\n",
      "[libx264 @ 0x55a7921a0e80] coded y,uvDC,uvAC intra: 37.0% 66.3% 8.0% inter: 19.1% 31.5% 0.6%\n",
      "[libx264 @ 0x55a7921a0e80] i16 v,h,dc,p:  9% 10%  2% 79%\n",
      "[libx264 @ 0x55a7921a0e80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 15% 13% 19%  7% 12%  9% 11%  7%  7%\n",
      "[libx264 @ 0x55a7921a0e80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 21% 19%  5% 10%  6%  7%  4%  4%\n",
      "[libx264 @ 0x55a7921a0e80] i8c dc,h,v,p: 51% 19% 19% 10%\n",
      "[libx264 @ 0x55a7921a0e80] Weighted P-Frames: Y:33.0% UV:26.6%\n",
      "[libx264 @ 0x55a7921a0e80] ref P L0: 51.4% 13.0% 22.4% 10.4%  2.7%\n",
      "[libx264 @ 0x55a7921a0e80] ref B L0: 82.6% 13.3%  4.0%\n",
      "[libx264 @ 0x55a7921a0e80] ref B L1: 92.3%  7.7%\n",
      "[libx264 @ 0x55a7921a0e80] kb/s:635.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num: 160\n",
      "total time(ms):0:00:39.278794\n",
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output.mp4': No such file or directory\n",
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:00:05.96, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55fcc0f06e80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55fcc0f06e80] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55fcc0f06e80] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 25 fps, 12800 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x55fcc0f05f00] video:671kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.317503%\n",
      "frame=  149 fps=0.0 q=-1.0 Lsize=     674kB time=00:00:05.84 bitrate= 944.8kbits/s speed=5.99x    \n",
      "[libx264 @ 0x55fcc0f06e80] frame I:11    Avg QP:21.79  size:  8224\n",
      "[libx264 @ 0x55fcc0f06e80] frame P:76    Avg QP:24.67  size:  4659\n",
      "[libx264 @ 0x55fcc0f06e80] frame B:62    Avg QP:25.56  size:  3908\n",
      "[libx264 @ 0x55fcc0f06e80] consecutive B-frames: 42.3%  4.0%  8.1% 45.6%\n",
      "[libx264 @ 0x55fcc0f06e80] mb I  I16..4: 24.0% 66.8%  9.3%\n",
      "[libx264 @ 0x55fcc0f06e80] mb P  I16..4: 15.4% 21.3%  3.3%  P16..4: 37.9%  6.6%  2.2%  0.0%  0.0%    skip:13.2%\n",
      "[libx264 @ 0x55fcc0f06e80] mb B  I16..4:  1.8%  5.3%  1.2%  B16..8: 42.2%  8.0%  1.1%  direct: 8.8%  skip:31.7%  L0:43.4% L1:52.3% BI: 4.3%\n",
      "[libx264 @ 0x55fcc0f06e80] 8x8 transform intra:57.7% inter:88.0%\n",
      "[libx264 @ 0x55fcc0f06e80] coded y,uvDC,uvAC intra: 41.8% 73.1% 17.6% inter: 24.4% 48.0% 0.6%\n",
      "[libx264 @ 0x55fcc0f06e80] i16 v,h,dc,p: 17% 19%  3% 61%\n",
      "[libx264 @ 0x55fcc0f06e80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 16% 19%  7% 10%  9%  9%  6%  7%\n",
      "[libx264 @ 0x55fcc0f06e80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 27% 19%  4%  7%  4%  5%  2%  2%\n",
      "[libx264 @ 0x55fcc0f06e80] i8c dc,h,v,p: 43% 23% 22% 12%\n",
      "[libx264 @ 0x55fcc0f06e80] Weighted P-Frames: Y:1.3% UV:0.0%\n",
      "[libx264 @ 0x55fcc0f06e80] ref P L0: 64.4%  8.5% 18.7%  8.2%  0.2%\n",
      "[libx264 @ 0x55fcc0f06e80] ref B L0: 86.9% 10.5%  2.6%\n",
      "[libx264 @ 0x55fcc0f06e80] ref B L1: 96.5%  3.5%\n",
      "[libx264 @ 0x55fcc0f06e80] kb/s:921.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image num: 1536\n",
      "total time(ms):0:06:26.785671\n",
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output.mp4': No such file or directory\n",
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:01:01.16, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55ef67fd5e80] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55ef67fd5e80] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55ef67fd5e80] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 25 fps, 12800 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x55ef67fd4f00] video:5138kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.339262%\n",
      "frame= 1529 fps=168 q=-1.0 Lsize=    5156kB time=00:01:01.04 bitrate= 691.9kbits/s speed=6.71x    \n",
      "[libx264 @ 0x55ef67fd5e80] frame I:35    Avg QP:21.18  size:  7754\n",
      "[libx264 @ 0x55ef67fd5e80] frame P:577   Avg QP:24.20  size:  4299\n",
      "[libx264 @ 0x55ef67fd5e80] frame B:917   Avg QP:24.40  size:  2736\n",
      "[libx264 @ 0x55ef67fd5e80] consecutive B-frames: 14.5% 12.8% 11.8% 61.0%\n",
      "[libx264 @ 0x55ef67fd5e80] mb I  I16..4: 27.9% 61.9% 10.1%\n",
      "[libx264 @ 0x55ef67fd5e80] mb P  I16..4: 21.4% 24.7%  2.6%  P16..4: 34.8%  5.5%  2.1%  0.0%  0.0%    skip: 8.9%\n",
      "[libx264 @ 0x55ef67fd5e80] mb B  I16..4:  4.8%  4.6%  0.7%  B16..8: 41.0%  4.9%  0.5%  direct: 8.8%  skip:34.6%  L0:47.5% L1:49.5% BI: 2.9%\n",
      "[libx264 @ 0x55ef67fd5e80] 8x8 transform intra:50.5% inter:93.2%\n",
      "[libx264 @ 0x55ef67fd5e80] coded y,uvDC,uvAC intra: 29.0% 63.8% 7.9% inter: 19.0% 48.0% 0.6%\n",
      "[libx264 @ 0x55ef67fd5e80] i16 v,h,dc,p: 12% 12%  2% 74%\n",
      "[libx264 @ 0x55ef67fd5e80] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 20% 14% 19%  6% 11% 10%  9%  6%  5%\n",
      "[libx264 @ 0x55ef67fd5e80] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 34% 24% 19%  3%  6%  4%  4%  2%  2%\n",
      "[libx264 @ 0x55ef67fd5e80] i8c dc,h,v,p: 51% 18% 22%  9%\n",
      "[libx264 @ 0x55ef67fd5e80] Weighted P-Frames: Y:14.7% UV:11.6%\n",
      "[libx264 @ 0x55ef67fd5e80] ref P L0: 62.9%  8.3% 19.0%  9.0%  0.8%\n",
      "[libx264 @ 0x55ef67fd5e80] ref B L0: 82.6% 14.2%  3.3%\n",
      "[libx264 @ 0x55ef67fd5e80] ref B L1: 95.0%  5.0%\n",
      "[libx264 @ 0x55ef67fd5e80] kb/s:688.17\n"
     ]
    }
   ],
   "source": [
    "# video test\n",
    "\n",
    "def get_bounding_boxes(probability_map, threshold=0.4):\n",
    "    \"\"\"\n",
    "    확률값이 threshold 이상인 픽셀들을 Bounding Box로 감싸기\n",
    "    \"\"\"\n",
    "    # 확률 맵을 0~255로 정규화 후 이진화 (Threshold 적용)\n",
    "    binary_mask = (probability_map > threshold).astype(np.uint8) * 255\n",
    "\n",
    "    # Contour(외곽선) 찾기\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Bounding Boxes 저장\n",
    "    bounding_boxes = [cv2.boundingRect(cnt) for cnt in contours]\n",
    "\n",
    "    return bounding_boxes\n",
    "\n",
    "def extract_frames(video_path, output_folder, fps=5):\n",
    "    # 비디오 캡처 객체 생성\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # 원본 비디오의 FPS 및 총 프레임 수 가져오기\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # 프레임을 저장할 폴더 생성\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    frame_interval = video_fps // fps  # 몇 프레임마다 저장할지 계산\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{saved_count:05d}.png\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames and saved to {output_folder}\")\n",
    "\n",
    "\n",
    "video_file = \"video5.mp4\"  # MP4 파일 경로\n",
    "# video_image_dir = \"bleeding_test_1/\"  # 저장할 image folder\n",
    "# video_image_dir = \"test_riwo/video2/\"  # 저장할 image folder\n",
    "video_image_dir = \"test_riwo/video5/\"  # 저장할 image folder\n",
    "\n",
    "\n",
    "yolo_video_image_dir = \"test_riwo/video2_yolo/\"  # 저장할 image folder\n",
    "\n",
    "# extract_frames(video_file, video_image_dir)\n",
    "\n",
    "\n",
    "# output_dir = \"bleeding_test_result_1\"\n",
    "# output_dir = \"test_riwo/video2_result_new/\"\n",
    "output_dir = \"test_riwo/video5_result_full/\"\n",
    "\n",
    "video_image_dirs = [\n",
    "    \"test_riwo/video5_important_image/\", \"test_riwo/video5_important_image2/\",\n",
    "    #\"test_riwo/video2/\", \"test_riwo/video5/\", # \"video1_image/\", \"video2_image/\", \"video3_image/\", \"video4_image/\", \"video5_image/\",\n",
    "    \"bleeding_test_1/\", \"bleeding_test_2/\", \"bleeding_test_3/\"\n",
    "]\n",
    "output_dirs = [\n",
    "    \"test_riwo/video5_important_output1/\", \"test_riwo/video5_important_output2/\",\n",
    "    #\"test_riwo/video2_output/\", \"test_riwo/video5_output/\", # \"video1_output/\", \"video2_output/\", \"video3_output/\", \"video4_output/\", \"video5_output/\",\n",
    "    \"bleeding_test_1_output/\", \"bleeding_test_2_output/\", \"bleeding_test_3_output/\"\n",
    "]\n",
    "offsets = [\n",
    "    0, 0, \n",
    "    #280, 0, # 280, 280, 280, 0, 0,\n",
    "    840, 840, 840\n",
    "]\n",
    "resols = [\n",
    "    \"low\", \"low\",\n",
    "    #\"low\", \"low\", # \"low\", \"low\", \"low\", \"low\", \"low\", \n",
    "    \"high\", \"high\", \"high\"\n",
    "]\n",
    "\n",
    "def test_video(video_image_dir, output_dir, x_offset, resol):\n",
    "    output_dir = os.path.join(\"test_new/\", output_dir)\n",
    "    \n",
    "    video_image_files = sorted([os.path.join(video_image_dir, f) for f in os.listdir(video_image_dir) if f.endswith(('.jpeg', '.png', '.jpg'))])  # 이미지 파일 리스트\n",
    "    # yolo_video_image_files = sorted([os.path.join(yolo_video_image_dir, f) for f in os.listdir(yolo_video_image_dir) if f.endswith(('.jpeg', '.png', '.jpg'))])  # 이미지 파일 리스트\n",
    "    \n",
    "    # test 데이터셋 및 DataLoader 생성\n",
    "    if resol == \"low\":\n",
    "        video_test_dataset = BleedingDatasetTest(video_image_files, x_offset=x_offset, transform=transform, augmentation=False)\n",
    "    else:\n",
    "        video_test_dataset = BleedingDatasetTestVideo(video_image_files, x_offset=x_offset, transform=transform, augmentation=False)\n",
    "\n",
    "    video_test_dataloader = DataLoader(video_test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # 모델 로드 (ResNet50 기반)\n",
    "    # model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "    \n",
    "    # 출력 채널 변경 (COCO 클래스 → predict 클래스)\n",
    "    # model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "    \n",
    "    # 🔹 Custom Backbone을 사용한 DeepLabV3 모델 정의\n",
    "    custom_backbone = CustomBackBone()\n",
    "    \n",
    "    # 🔹 DeepLabV3 모델에 Custom Backbone 연결\n",
    "    model = DeepLabV3(\n",
    "        backbone=custom_backbone,\n",
    "        classifier=DeepLabHead(custom_backbone.out_channels, num_classes)  # classifier의 입력 크기를 backbone에 맞춤\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"deeplabv3_bleeding_self_attention_new.pth\"))\n",
    "    \n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_num = 0\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    for images in video_test_dataloader:\n",
    "        \n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(images)[\"out\"]  # DeepLabV3의 출력 가져오기\n",
    "            images = images[:, 1, :, :, :]\n",
    "        \n",
    "        for image, pred in zip(images, preds):\n",
    "            total_num += 1\n",
    "            \n",
    "            # 📌 후처리 (Threshold)\n",
    "            pred = F.softmax(pred, dim=0)\n",
    "            max_probs, pred_mask = torch.max(pred, dim=0)  # (512, 512)\n",
    "    \n",
    "            # threshold = 0.4\n",
    "            # pred_mask[max_probs < threshold] = 0\n",
    "    \n",
    "            # NumPy 변환\n",
    "            pred_mask = pred_mask.cpu().numpy()\n",
    "            \n",
    "            # 원본 이미지, 마스크 변환\n",
    "            original_image = image.cpu().numpy().transpose(1,2,0)\n",
    "            original_image = (original_image * 255).astype(np.uint8)  # 정규화 해제\n",
    "           \n",
    "            # ✅ 컬러맵 적용 (GT = Green, Pred = Red, Overlap = Yellow)\n",
    "            overlay = np.array(original_image)\n",
    "            overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "            green = [0, 255, 0]  # Prediction - Green\n",
    "            red = [0, 0, 255]  # Prediction - Red\n",
    "            blue = [255, 0, 0]  # Prediction - Blue\n",
    "    \n",
    "            yellow = [0, 255, 255]  # Overlapping - Yellow\n",
    "            BF_color = [0, 237, 204] # BF color in YOLO\n",
    "    \n",
    "            '''\n",
    "            mask_layer = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "            mask_layer[pred_mask == 1] = green  # Prediction\n",
    "            '''\n",
    "    \n",
    "            # 결과 마스크 생성\n",
    "            # thresholded_mask_layer = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "            # thresholded_mask_layer = np.zeros_like(overlay, dtype=np.uint8)\n",
    "            # thresholded_mask_layer = cv2.applyColorMap(np.uint8(pred[1].cpu() * 255), cv2.COLORMAP_JET)\n",
    "            \n",
    "            # 확률이 높은 영역에 대한 Bounding Box 찾기\n",
    "            bounding_boxes = get_bounding_boxes(pred[1].cpu().numpy(), threshold=0.5)\n",
    "            \n",
    "            for (x, y, w, h) in bounding_boxes:\n",
    "                cv2.rectangle(overlay, (x, y), (x + w, y + h), BF_color, 2)\n",
    "                # 텍스트 위치 설정 (Bounding Box의 오른쪽 위)\n",
    "                text_x = x + w - 30  # 오른쪽 끝 - 30px\n",
    "                text_y = y + 15  # 위쪽 여백 고려\n",
    "                \n",
    "                # 텍스트 바탕 박스 (검은색)\n",
    "                cv2.rectangle(overlay, (text_x - 2, text_y - 12), \n",
    "                              (text_x + 22, text_y + 3), (0, 0, 0), -1)\n",
    "        \n",
    "                # 'BF' 텍스트 추가 (노란색)\n",
    "                cv2.putText(overlay, \"BF\", (text_x, text_y), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "            '''\n",
    "            # 설정값\n",
    "            grid_size = 4  # 4x4 그리드\n",
    "            cell_size = 512 // grid_size  # 각 셀의 크기\n",
    "            binary_pred_mask = (pred_mask == 1).astype(np.uint8)\n",
    "    \n",
    "            for i in range(grid_size):\n",
    "                for j in range(grid_size):\n",
    "                    # 각 셀의 위치 계산\n",
    "                    x_start, x_end = i * cell_size, (i + 1) * cell_size\n",
    "                    y_start, y_end = j * cell_size, (j + 1) * cell_size\n",
    "                    \n",
    "                    # 셀 내부의 마스크 값이 1인 픽셀의 비율 계산\n",
    "                    cell = binary_pred_mask[x_start:x_end, y_start:y_end]\n",
    "                    mask_ratio = np.mean(cell)\n",
    "                    \n",
    "                    # 마스크 비율 threshold\n",
    "                    threshold = 0.001\n",
    "    \n",
    "                    if (i == 0 or i == grid_size - 1) and (j == 0 or j == grid_size - 1): # corner\n",
    "                        threshold = 0.001\n",
    "                    elif (i == 0 or i == grid_size - 1) or (j == 0 or j == grid_size - 1): # side\n",
    "                        threshold = 0.001\n",
    "                    \n",
    "                    if mask_ratio > threshold:\n",
    "                        thresholded_mask_layer[x_start:x_end, y_start:y_end] = green\n",
    "            \n",
    "            thresholded_mask_layer[pred_mask==1] = BF_color\n",
    "            '''\n",
    "    \n",
    "            # 512x512 -> 1080x1080 업샘플링 (cubic interpolation 사용)\n",
    "            # image_1080 = cv2.resize(thresholded_mask_layer, (1080, 1080), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "            '''\n",
    "            # 1920x1080 빈 이미지 생성 (검은색 배경)\n",
    "            image_1920x1080 = np.zeros((1080, 1920, 3), dtype=np.uint8)\n",
    "            \n",
    "            # 1080x1080 이미지를 1920x1080 이미지의 가로 280~1360 위치에 배치\n",
    "            x_offset = 0  # 왼쪽 시작점\n",
    "            y_offset = 0    # 세로 정렬 (상단부터 채우기)\n",
    "            \n",
    "            # 이미지 삽입\n",
    "            image_1920x1080[y_offset:y_offset + 1080, x_offset:x_offset + 1080] = image_1080\n",
    "            '''\n",
    "            \n",
    "            # ✅ 최종 합성\n",
    "            # blended = cv2.addWeighted(overlay, 0.8, thresholded_mask_layer, 0.4, 0)\n",
    "            blended = overlay\n",
    "    \n",
    "            # blended = cv2.resize(blended, (1920, 1080), interpolation=cv2.INTER_LANCZOS4)\n",
    "    \n",
    "            # ✅ 저장\n",
    "            filename = f\"output_{total_num:04d}.png\"\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            cv2.imwrite(output_path, blended)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    print(\"image num: \" + str(len(video_test_dataloader) * 16))\n",
    "    print(\"total time(ms):\", end='')\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "    os.system(f\"cd {output_dir} && rm output.mp4\")\n",
    "    os.system(f\"cd {output_dir} && ffmpeg -framerate 25 -i output_%04d.png -c:v libx264 -pix_fmt yuv420p output.mp4 & cd ..\")\n",
    "    print(\"video test completed\")\n",
    "\n",
    "for video_image_dir, output_dir, offset, resol in zip(video_image_dirs, output_dirs, offsets, resols):\n",
    "    test_video(video_image_dir, output_dir, offset, resol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e4e9e-e938-438b-8947-1323b4db38cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c5920-c661-48c4-83a3-1c7493d66a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
