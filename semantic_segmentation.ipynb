{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58fac117-6c69-4c60-826c-18bd02c2e998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/bf_optical_flow/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f90a52-8da8-4f99-bcf0-ed989f5441e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8]  # Gamma values\n",
    "\n",
    "def transform(img):\n",
    "    return img\n",
    "\n",
    "def gamma_correction(img, gamma):\n",
    "    return TF.adjust_gamma(img, gamma)\n",
    "\n",
    "flip_types = [\"horizontal\", \"vertical\", \"both\", \"none\"]\n",
    "\n",
    "def random_flip(img, flip_type):\n",
    "    if flip_type == \"horizontal\":\n",
    "        return TF.hflip(img)\n",
    "    elif flip_type == \"vertical\":\n",
    "        return TF.vflip(img)\n",
    "    elif flip_type == \"both\":\n",
    "        return TF.hflip(TF.vflip(img))\n",
    "    return img  # No flip\n",
    "\n",
    "# {'Instrument', 'Care', 'Bubble', 'unkown', 'unknown', 'Fat', 'SoftTIssue', 'Dura', 'BF', 'SoftTissue', 'vessel', 'Vessel', 'Bone', 'LF', 'SofrTissue'}\n",
    "num_classes = 11 # Background, BF, Vessel, Instrument, Care, Bubble, Fat, Bone, LF, Dura, SoftTissue\n",
    "\n",
    "# 클래스별 라벨 매핑\n",
    "class_map = {\"BF\": 1, \"Vessel\": 2, \"vessel\": 2, \"Instrument\": 3, \"Care\": 4, \"Bubble\": 5, \"Fat\": 6, \"Bone\": 7, \"LF\": 8, \"Dura\": 9, \"SoftTissue\": 10, \"SofrTissue\": 10, \"SoftTIssue\": 10}\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BleedingDataset(Dataset):\n",
    "    def __init__(self, image_files, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.json_paths = [f.replace('.jpeg', '.json').replace('.png', '.json') for f in self.image_paths]\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 원본 이미지 로드\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.crop((420, 0, 1500, 1080))\n",
    "\n",
    "        # JSON 파일 로드\n",
    "        json_path = self.json_paths[idx]\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # 빈 마스크 생성\n",
    "        mask = np.zeros((data[\"imageHeight\"], data[\"imageWidth\"]), dtype=np.uint8)\n",
    "\n",
    "        # 출혈(BF) 영역 폴리곤 마스크 생성\n",
    "        for shape in data[\"shapes\"]:\n",
    "            label = shape[\"label\"]\n",
    "            if label in class_map:\n",
    "                points = np.array(shape[\"points\"], dtype=np.int32)\n",
    "                cv2.fillPoly(mask, [points], class_map[label])\n",
    "\n",
    "        \n",
    "        # PIL 이미지 변환 후 Tensor 변환\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = mask.crop((420, 0, 1500, 1080))\n",
    "        \n",
    "        image = self.transform_image(image)\n",
    "        mask = self.transform_mask(mask)\n",
    "\n",
    "        if self.augmentation:\n",
    "            gamma = random.choice(gamma_values)  # 랜덤한 gamma 값 선택\n",
    "            image = gamma_correction(image, gamma)\n",
    "            #mask = gamma_correction(mask, gamma)\n",
    "            flip_type = random.choice(flip_types)\n",
    "            image = random_flip(image, random_flip)\n",
    "            mask = random_flip(mask, random_flip)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BleedingDatasetTest(Dataset):\n",
    "    def __init__(self, image_files, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 원본 이미지 로드\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.crop((420, 0, 1500, 1080))\n",
    "        \n",
    "        image = self.transform_image(image)\n",
    "        # image = self.toTensor(image)\n",
    "    \n",
    "        return image\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class BleedingDatasetTestVideo(Dataset):\n",
    "    def __init__(self, image_files, transform=None, augmentation=False):\n",
    "        self.image_paths = image_files\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.transform_image = transform_image\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 원본 이미지 로드\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = image.crop((840, 0, 3000, 2160))\n",
    "        \n",
    "        image = self.transform_image(image)\n",
    "        # image = self.toTensor(image)\n",
    "    \n",
    "        return image\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),  # 일반 이미지용,\n",
    "    transforms.ToTensor(),\n",
    "    #lambda x: x.long(),\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.NEAREST),  # mask 용,\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x * 255,  # 다시 255를 곱하여 (0,255) 범위로 변환\n",
    "    lambda x: x.long(),\n",
    "])\n",
    "\n",
    "# Dice Loss 정의\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = F.softmax(pred, dim=1)  # 여러 클래스 예측 확률로 변환\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)  # One-hot encoding\n",
    "    intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "    union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()  # 다중 클래스 dice loss\n",
    "\n",
    "def focal_loss(pred, target, gamma=2.0):\n",
    "    pred = F.softmax(pred, dim=1)  # 확률 분포\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)\n",
    "    ce_loss = -(target_onehot * torch.log(pred + 1e-6))  # Cross Entropy 기반\n",
    "    focal_loss = (1 - pred) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "def iou_loss(pred, target, smooth=1e-6):\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2)\n",
    "    intersection = (pred * target_onehot).sum(dim=(2,3))\n",
    "    union = pred.sum(dim=(2,3)) + target_onehot.sum(dim=(2,3)) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return 1 - iou.mean()\n",
    "\n",
    "def loss_fn(pred, target):\n",
    "    return (dice_loss(pred, target) + focal_loss(pred, target) + iou_loss(pred, target)) / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed21a7c3-cd1b-472d-b3b3-93d226e5afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"0014_spine_endoscope_data/\"\n",
    "image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpeg', '.png'))])  # 이미지 파일 리스트\n",
    "\n",
    "# ✅ Train Test Split (85:15 비율)\n",
    "train_images, test_images = train_test_split(image_files, test_size=0.15, random_state=42)\n",
    "\n",
    "# train 데이터셋 및 DataLoader 생성\n",
    "train_dataset = BleedingDataset(train_images, transform=transform, augmentation=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# test 데이터셋 및 DataLoader 생성\n",
    "test_dataset = BleedingDataset(test_images, transform=transform, augmentation=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34cfa73-959e-41f8-97c7-841efc2f479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4018\n",
      "Epoch [2/10], Loss: 0.3655\n",
      "Epoch [3/10], Loss: 0.3620\n",
      "Epoch [4/10], Loss: 0.3587\n",
      "Epoch [5/10], Loss: 0.3541\n",
      "Epoch [6/10], Loss: 0.3529\n",
      "Epoch [7/10], Loss: 0.3504\n",
      "Epoch [8/10], Loss: 0.3477\n",
      "Epoch [9/10], Loss: 0.3449\n",
      "Epoch [10/10], Loss: 0.3427\n",
      "모델 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 로드 (ResNet50 기반)\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "# 출력 채널 변경 (COCO 클래스 → predict 클래스)\n",
    "model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "model.load_state_dict(torch.load(\"deeplabv3_bleeding_multiclass_crop.pth\"))\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)[\"out\"]  # DeepLabV3의 출력 가져오기\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), \"deeplabv3_bleeding_multiclass_crop.pth\")\n",
    "print(\"모델 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d53d084-fcea-44b8-be32-e7eb4ef9c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 모델 로드 (ResNet50 기반)\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "# 출력 채널 변경 (COCO 클래스 → predict 클래스)\n",
    "model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "model.load_state_dict(torch.load(\"deeplabv3_bleeding_multiclass_crop.pth\"))\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "output_dir = \"test_results/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "total_num = 0\n",
    "for images, masks in test_dataloader:\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = model(images)[\"out\"]  # DeepLabV3의 출력 가져오기\n",
    "        \n",
    "    for image, mask, pred in zip(images, masks, preds):\n",
    "        total_num += 1\n",
    "\n",
    "        # 📌 후처리 (Sigmoid + Threshold)\n",
    "        pred_mask = torch.argmax(pred, dim=0).cpu().numpy()  # (512, 512)\n",
    "        original_mask = mask.squeeze().cpu().numpy()  # GT 마스크 (512, 512)\n",
    "        \n",
    "        # 원본 이미지, 마스크 변환\n",
    "        original_image = image.cpu().numpy().transpose(1,2,0)\n",
    "        original_image = (original_image * 255).astype(np.uint8)  # 정규화 해제\n",
    "        \n",
    "        # ✅ 컬러맵 적용 (GT = Green, Pred = Red, Overlap = Yellow)\n",
    "        overlay = np.array(original_image)\n",
    "        overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        green = [0, 255, 0]  # Ground Truth (GT) - Green\n",
    "        red = [0, 0, 255]  # Prediction - Red\n",
    "        yellow = [0, 255, 255]  # Overlapping - Yellow\n",
    "\n",
    "        mask_layer = np.zeros_like(overlay, dtype=np.uint8)\n",
    "        mask_layer[original_mask == 1] = green  # GT\n",
    "        mask_layer[pred_mask == 1] = red  # Prediction\n",
    "        mask_layer[(original_mask == 1) & (pred_mask == 1)] = yellow  # Overlap\n",
    "\n",
    "        # ✅ 최종 합성\n",
    "        blended = cv2.addWeighted(overlay, 0.7, mask_layer, 0.5, 0)\n",
    "\n",
    "        # blended = cv2.resize(blended, (1920, 1080), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "        # ✅ 저장\n",
    "        filename = f\"output_{total_num}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, blended)\n",
    "        \n",
    "print(\"test completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae1f41b-527f-4c38-b9bb-874d87e14b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1-3ubuntu5 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 13 (Ubuntu 13.2.0-23ubuntu3)\n",
      "  configuration: --prefix=/usr --extra-version=3ubuntu5 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --disable-omx --enable-gnutls --enable-libaom --enable-libass --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libharfbuzz --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-openal --enable-opencl --enable-opengl --disable-sndio --enable-libvpl --disable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-ladspa --enable-libbluray --enable-libjack --enable-libpulse --enable-librabbitmq --enable-librist --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libx264 --enable-libzmq --enable-libzvbi --enable-lv2 --enable-sdl2 --enable-libplacebo --enable-librav1e --enable-pocketsphinx --enable-librsvg --enable-libjxl --enable-shared\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output_%04d.png':\n",
      "  Duration: 00:00:06.00, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/unknown/unknown), 512x512, 10 fps, 10 tbr, 10 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x558f081c93c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x558f081c93c0] profile High, level 2.2, 4:2:0, 8-bit\n",
      "[libx264 @ 0x558f081c93c0] 264 - core 164 r3108 31e19f9 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x512, q=2-31, 10 fps, 10240 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0x558f081c8440] video:49kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 3.130452%\n",
      "frame=   60 fps=0.0 q=-1.0 Lsize=      51kB time=00:00:05.70 bitrate=  73.0kbits/s speed=  31x    \n",
      "[libx264 @ 0x558f081c93c0] frame I:1     Avg QP: 9.56  size:  3237\n",
      "[libx264 @ 0x558f081c93c0] frame P:16    Avg QP:16.59  size:  1404\n",
      "[libx264 @ 0x558f081c93c0] frame B:43    Avg QP:22.05  size:   559\n",
      "[libx264 @ 0x558f081c93c0] consecutive B-frames:  3.3%  3.3%  0.0% 93.3%\n",
      "[libx264 @ 0x558f081c93c0] mb I  I16..4:  2.1% 93.1%  4.8%\n",
      "[libx264 @ 0x558f081c93c0] mb P  I16..4:  1.0%  3.4%  0.8%  P16..4:  8.3%  2.4%  1.2%  0.0%  0.0%    skip:82.8%\n",
      "[libx264 @ 0x558f081c93c0] mb B  I16..4:  0.1%  0.3%  0.1%  B16..8:  6.3%  1.3%  0.3%  direct: 0.9%  skip:90.7%  L0:46.2% L1:46.8% BI: 6.9%\n",
      "[libx264 @ 0x558f081c93c0] 8x8 transform intra:77.6% inter:78.4%\n",
      "[libx264 @ 0x558f081c93c0] coded y,uvDC,uvAC intra: 34.7% 52.8% 27.0% inter: 3.0% 5.1% 0.3%\n",
      "[libx264 @ 0x558f081c93c0] i16 v,h,dc,p: 11% 11%  8% 70%\n",
      "[libx264 @ 0x558f081c93c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 50% 14% 13%  3%  5%  4%  4%  4%  4%\n",
      "[libx264 @ 0x558f081c93c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 15% 27%  6%  7%  6%  6%  4%  4%\n",
      "[libx264 @ 0x558f081c93c0] i8c dc,h,v,p: 68% 12% 11%  9%\n",
      "[libx264 @ 0x558f081c93c0] Weighted P-Frames: Y:18.8% UV:18.8%\n",
      "[libx264 @ 0x558f081c93c0] ref P L0: 66.0% 10.8% 12.8%  8.7%  1.7%\n",
      "[libx264 @ 0x558f081c93c0] ref B L0: 86.0% 12.2%  1.9%\n",
      "[libx264 @ 0x558f081c93c0] ref B L1: 95.2%  4.8%\n",
      "[libx264 @ 0x558f081c93c0] kb/s:66.33\n"
     ]
    }
   ],
   "source": [
    "# video test\n",
    "\n",
    "def extract_frames(video_path, output_folder, fps=20):\n",
    "    # 비디오 캡처 객체 생성\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # 원본 비디오의 FPS 및 총 프레임 수 가져오기\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # 프레임을 저장할 폴더 생성\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    frame_interval = video_fps // fps  # 몇 프레임마다 저장할지 계산\n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{saved_count:05d}.png\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {saved_count} frames and saved to {output_folder}\")\n",
    "\n",
    "# 사용 예시\n",
    "video_file = \"bleeding_test_1.mp4\"  # MP4 파일 경로\n",
    "video_image_dir = \"bleeding_test_3/\"  # 저장할 폴더\n",
    "# extract_frames(video_file, video_image_dir)\n",
    "\n",
    "output_dir = \"bleeding_test_result_3/\"\n",
    "\n",
    "video_image_files = sorted([os.path.join(video_image_dir, f) for f in os.listdir(video_image_dir) if f.endswith(('.jpeg', '.png'))])  # 이미지 파일 리스트\n",
    "\n",
    "# test 데이터셋 및 DataLoader 생성\n",
    "video_test_dataset = BleedingDatasetTestVideo(video_image_files, transform=transform, augmentation=False)\n",
    "video_test_dataloader = DataLoader(video_test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "# 모델 로드 (ResNet50 기반)\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "# 출력 채널 변경 (COCO 클래스 → predict 클래스)\n",
    "model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "model.load_state_dict(torch.load(\"deeplabv3_bleeding_multiclass_crop.pth\"))\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "total_num = 0\n",
    "for images in video_test_dataloader:\n",
    "    images = images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = model(images)[\"out\"]  # DeepLabV3의 출력 가져오기\n",
    "        \n",
    "    for image, pred in zip(images, preds):\n",
    "        total_num += 1\n",
    "        \n",
    "        # 📌 후처리 (Sigmoid + Threshold)\n",
    "        pred_mask = torch.argmax(pred, dim=0).cpu().numpy()  # (512, 512)\n",
    "        \n",
    "        # 원본 이미지, 마스크 변환\n",
    "        original_image = image.cpu().numpy().transpose(1,2,0)\n",
    "        original_image = (original_image * 255).astype(np.uint8)  # 정규화 해제\n",
    "       \n",
    "        # ✅ 컬러맵 적용 (GT = Green, Pred = Red, Overlap = Yellow)\n",
    "        overlay = np.array(original_image)\n",
    "        overlay = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        green = [0, 255, 0]  # Ground Truth (GT) - Green\n",
    "        red = [0, 0, 255]  # Prediction - Red\n",
    "        yellow = [0, 255, 255]  # Overlapping - Yellow\n",
    "\n",
    "        mask_layer = np.zeros_like(overlay, dtype=np.uint8)\n",
    "        mask_layer[pred_mask == 1] = green  # Prediction\n",
    "\n",
    "        # ✅ 최종 합성\n",
    "        blended = cv2.addWeighted(overlay, 0.7, mask_layer, 0.5, 0)\n",
    "\n",
    "        # blended = cv2.resize(blended, (1920, 1080), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "        # ✅ 저장\n",
    "        filename = f\"output_{total_num:04d}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(output_path, blended)\n",
    "\n",
    "os.system(f\"cd {output_dir} && ffmpeg -framerate 10 -i output_%04d.png -c:v libx264 -pix_fmt yuv420p output.mp4 & cd ..\")\n",
    "print(\"video test completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac861d73-357d-4fc5-a491-91cc7d6c2b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
